# Your rights & routes

You shouldn’t have to surrender your privacy to be an adult online. Good systems prove your age, then let the data go. And if a platform makes bad choices, you have ways to push back. This chapter is your pocket guide to deletion, alternatives, and where to complain when something’s off.

<details>
<summary><strong>What happens to my photo/ID? Can I make them delete it?</strong></summary>
Certified providers should delete images immediately after the check and keep only what’s needed (e.g., an “18+ OK” token tied to your account). Under UK data protection law (UK GDPR), you have rights to access, correction, and erasure where applicable.

In practice: good providers state deletion clearly and explain retention for audit tokens. If a service claims deletion, they should be able to show it. You can exercise your rights with the provider (data access/erasure) and, if needed, complain to the ICO. See the [UK GDPR overview (ICO)](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/) and Ofcom’s [online safety hub](https://www.ofcom.org.uk/online-safety) for how these regimes interact. ICO materials updated in 2025 emphasise immediate deletion for age‑assurance images and minimal data handling.

Tip: look for independent certification and audits. For example, the <strong>Age Check Certification Scheme (ACCS)</strong> audits age‑assurance providers ([ACCS](https://accscheme.com/)), the <strong>Digital Identity and Attributes Trust Framework (DIATF)</strong> recognises certified identity/attribute providers ([GOV.UK—DIATF](https://www.gov.uk/government/collections/digital-identity-and-attributes-trust-framework)), and the <strong>PASS</strong> scheme certifies proofs of age ([PASS](https://www.pass-scheme.org.uk/)). Providers such as <em>Yoti</em> publish certifications and audit summaries covering deletion and accuracy.
</details>

<details>
<summary><strong>How do I complain about a bad implementation?</strong></summary>
Start with the platform’s complaints route (required by the OSA). If ignored or the design is unfair (e.g., “credit‑card only”), escalate to the regulator.

Routes: (1) platform complaint/appeal, (2) Ofcom complaint about online safety issues (especially systemic ones), (3) ICO complaint if the issue is data protection (e.g., retention). We recommend Ofcom adopt a simpler user‑first intake that aggregates patterns so people don’t have to write long dossiers—but for now, use the current [Ofcom complaints](https://www.ofcom.org.uk/complaints) page and keep your evidence. For transparency duties that apply to larger/categorised services, Ofcom has final guidance on what reports must include (see [Transparency reporting—final guidance (PDF)](https://www.ofcom.org.uk/siteassets/resources/documents/consultations/category-1-10-weeks/consultation-draft-transparency-reporting-guidance/main-docs/final-transparency-guidance.pdf)).
</details>

<details>
<summary><strong>What if I don’t want to share ID at all?</strong></summary>
Ask for a non‑ID route: facial age estimation (with deletion), open banking (bank confirms “over 18”), mobile‑network checks, or a PASS digital proof of age.

Ofcom’s codes list multiple “highly effective” methods so you shouldn’t be forced into one route. If a service offers only one choice, challenge it using the platform complaint route and cite Ofcom’s [children’s codes](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/statement-protecting-children-from-harms-online).
</details>

<details>
<summary><strong>How do appeals and retries work if I’m mis‑aged?</strong></summary>
You should be able to retry quickly (better lighting/camera) or switch to a stronger fallback (ID+liveness, open banking, MNO, PASS) without being locked out.

A fair system explains the fallback ladder and offers a fast appeal with human review for edge cases (e.g., atypical faces, disability impacts). These expectations align with Ofcom’s fairness and reliability aims in the [children’s codes](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/statement-protecting-children-from-harms-online).
</details>

<details>
<summary><strong>Do small/federated communities have to do all this?</strong></summary>
Duties scale. Many small servers won’t hit heavy thresholds, but federated admins are still “providers” and must do proportionate basics.

Minimums worth doing: publish a simple safety policy (what’s allowed; reporting route), run a short risk assessment (what harms could appear; how you respond), and pick low‑friction age‑assurance options if you ever need them (email estimation, PASS). Keep short logs of decisions. Ofcom’s [roadmap](https://www.ofcom.org.uk/online-safety/illegal-and-harmful-content/roadmap-to-regulation) and [hub](https://www.ofcom.org.uk/online-safety) explain proportionality.
</details>

