# Methodology, Scoring & Taxonomy

<div class="tag-codes">

This section explains how we researched, structured and scored the items in A Year of Labour. It is written to be clear and readable for any motivated reader, not just policy specialists. Where we use short tags like `[impact-proven]` or `[horizon: short]`, think of them as labels that help us sort and analyse the content later.

We followed the writing principles in our style guide: plainspoken, direct and human. We aim to show the impact, understand tradeâ€‘offs, and make it easy to challenge or improve the work. If you spot something we should correct or expand, please open an issue or pull request [on the GitHub repository](https://github.com/Better-Britain/).

## What we researched

We gathered as many Labour policies and meaningful nonâ€‘policy events as we could find since the 2024 election. That includes most formal programmes and legislation, plus events and outcomes that are directly attributable to government decisions, delivery choices or public communications.

For each item, we described what happened, what was intended, and what evidence exists so far. We also captured competing positions when they were relevant, and linked to source material so you can check claims yourself.

## How we built the evidence base

Each issue started life as an entry 'deep research' reports, to which we added context through a mix of web search, manual reading of official documents, trusted reporting, and sector analyses, and further automated/assisted research. Main chats were exported and saved in this repository, and we augmented them with citations to reports, statements and data series. When sources disagreed, we tried to present both the disagreement and the reasons behind it.

## From research to policy cards

The research was organised into policy groups (for example, energy, housing or welfare). That structure helps readers find related work and see connections across departments.

For each policy or incident, we produced a short entry (a â€œcardâ€) with a consistent set of fields: a plainâ€‘language intent, the main mechanisms, key claims and evidence, costs and funding, distributional effects, risks and constraints, a brief timeline, and an outcome score. Wherever we make a claim, we include an inline citation like `[^id]` and list the references in the citations file so they can also be exported inline.

## Scoring process

Every card has an outcome score on a simple fiveâ€‘point scale from âˆ’2 to +2. The score is an informed judgement about the balance of likely benefits and harms at this point in time. It is not a prediction and it can change as delivery progresses.

- <span class="score-help score-2">`+2` means strong positive outcomes are visible or very likely if maintained.</span>
- <span class="score-help score-1">`+1` means modest positive outcomes or a sensible direction with caveats.</span>
- <span class="score-help score-0">`0` means unclear or balanced impacts; it is too early to tell.</span>
- <span class="score-help score--1">`âˆ’1` means modest negative outcomes or notable risks outweighing benefits.</span>
- <span class="score-help score--2">`âˆ’2` means serious negative outcomes or highâ€‘risk choices with little offsetting value.</span>

Initial scores were generated automatically based on tagged evidence. Every score was then reviewed by a human editor for context and fairness. All scores remain open to revision as new evidence arrives or different perspectives are offered. Contributions and reasoned challenges are welcome via GitHub.

## Scoring consistency

A broader range of scores was considered, and would give more opportunity for nuance in complex issues, but assigning a subjective score to every policy means we have to make decisions about which influences and outcomes are more important than others (does the financial cost of national energy independence outweigh the national security offered by relying less on overseas gas?), so a narrow range keeps the system simple and clear.

A weighted approach was also considered (is nuclear energy capacity more or less important than consistent building regulations around single-sex spaces?), but that very quickly shows the limitations, and enhances the bias, rather than reduces it. We're not trying to make any one perspective more important than any other, but to consider the overall effects of decisions made and the plans laid out.

We also considered testing policy performance against Conservative policies of the last few years (with post-COVID as the primary timespan), but the noise level in reporting was WAY too high for the time available to parse secondary/support issues, and the consistency in official documentation and analysis way too low, making reasonable analysis difficult.

Our scoring is intended to be seen as comparable to better or worse governance outcomes, reducing the overall potential range lets us define how broad this scale _could_ be, so we can focus on how and where it is relatively good (and where it isn't). 

Practically, our scale runs from "the worst any government could be expected to do" to "the best any government could be expected to do", given the range of issues tested, making it less a morality or political test, and more a test/scale of competence and effectiveness.

## Taxonomy and tags

<details>
<summary>
To make the content filterable and to keep the writeâ€‘ups short, we use a small set of tags. Expand for details.
</summary>
Impact quality describes how confident we are in a claim:

- âœ… `[impact-proven]` is supported by direct evidence, delivered outputs or robust evaluation.
- â†— `[impact-likely]` has strong rationale and early indicators but needs time to confirm.
- â—‡ `[impact-hypothetical]` is a plausible mechanism with little or no current evidence.
- ï¼Ÿ `[unknown]` flags a material uncertainty or missing data.
- ğŸ’¬ `[opinion]` marks a clearly interpretive statement.

Areas of effect help group by domain. Common examples include: âš¡ `[area: energy]`, ğŸ’§ `[area: water]`, ğŸ  `[area: housing]`, ğŸ—º `[area: planning]`, ğŸ¥ `[area: nhs]`, ğŸ§‘â€âš•ï¸ `[area: social-care]`, ğŸ‘ª `[area: welfare]`, ğŸ“ `[area: education]`, ğŸ§° `[area: skills]`, ğŸ›‚ `[area: migration]`, âš– `[area: justice]`, Â£ `[area: economy]`, ğŸ§¾ `[area: fiscal]`, ğŸ› `[area: devolution]`, ğŸ’» `[area: digital]`, ğŸ¤– `[area: ai]`, ğŸŒ¿ `[area: environment]`, ğŸ¢ `[area: business]`, ğŸ‘· `[area: labour-market]`, ğŸš† `[area: transport]`, ğŸ­ `[area: culture]`, ğŸŒ `[area: foreign]`, ğŸ›¡ `[area: defence]`.

Time horizons describe when effects should become visible: â± `[horizon: short]` (under 1 year), â³ `[horizon: medium]` (1â€“3 years), and ğŸ•° `[horizon: long]` (over 3 years).

Distributional tags highlight who is most affected. For example: ğŸ’· `[dist: low-income]`, ğŸ‘µ `[dist: pensioners]`, ğŸ”‘ `[dist: renters]`, ğŸ¡ `[dist: homeowners]`, ğŸª `[dist: SMEs]`, ğŸ¢ `[dist: large-firms]`, and regional tags like ğŸ“ `[dist: regions:Greater Manchester]`.

Risk and uncertainty tags call out the main constraints. We use âš– `[risk: legal]`, ğŸ— `[risk: delivery]`, Â£ `[risk: finance]`, ğŸ› `[risk: political]` and ğŸ“Š `[risk: data-gap]` to signpost what might go wrong or what we do not yet know.

Cards also have a small â€œmeta lineâ€ under the title. Common tags there include: âœ… `[status: enacted]` / ğŸ›  `[status: programme]`, ğŸ› `[lead: department/body]`, ğŸ“… `[start: YYYY-MM]`, and (again) the time horizon tags above.
</details>

## Citations and references

We use inline footnotes for every material claim: `[^id]`. Multiple references can be listed in one footnote, for example: `[^rshe-guidance-2025; ^ofcom-roadmap-2023]`. Source registries and provenance notes live in the references section of the project so you can trace what we relied on and why.

</div>

<p class="citations-toggle-row">
  <button type="button" id="toggle-citations" data-citations-visibility="toggle" class="btn">Show citation titles</button>
  <span id="citations-state" class="muted" aria-live="polite"></span>
</p>

## Build and outputs

Cards are stored as markdownâ€‘asâ€‘data. A small build pipeline compiles them into the public report and the summary table. That table offers an overview, but is not strictly analysis; it is a quick map that points you to the underlying cards.

Because the content is structured, we can add interactive features later (for example, filters by area, horizon or risk). If readers think those would be helpful, we will prioritise them.

## Style and voice

We aim for clarity over jargon, short paragraphs over dense blocks, and concrete examples over abstract claims. We focus on people and outcomes, not on pointâ€‘scoring. See the [Writing Voice & Style guide](./style-guide.html) for more detail on tone and presentation.

## Roadmap and open questions

Two nearâ€‘term improvements are on the list. 
- Ratings will be adjusted over time, and while updates may be limited, this report can develop into an ongoing 'report card' for Labour's government over time.
- Filtering/sorting based on tags of various types _might_ be useful.
