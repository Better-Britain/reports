# Methodology, Scoring & Taxonomy

<div class="tag-codes">

This section explains how we researched, structured and scored the items in A Year of Labour. It is written to be clear and readable for any motivated reader, not just policy specialists. Where we use short tags like `[impact-proven]` or `[horizon: short]`, think of them as labels that help us sort and analyse the content later.

We followed the writing principles in our style guide: plainspoken, direct and human. We aim to show the impact, understand trade‑offs, and make it easy to challenge or improve the work. If you spot something we should correct or expand, please open an issue or pull request on the GitHub repository.

## What we researched

We gathered as many Labour policies and meaningful non‑policy events as we could find since the 2024 election. That includes most formal programmes and legislation, plus events and outcomes that are directly attributable to government decisions, delivery choices or public communications.

For each item, we described what happened, what was intended, and what evidence exists so far. We also captured competing positions when they were relevant, and linked to source material so you can check claims yourself.

## How we built the evidence base

Each issue started life as an entry 'deep research' reports, to which we added context through a mix of web search, manual reading of official documents, trusted reporting, and sector analyses, and further automated/assisted research. Main chats were exported and saved in this repository, and we augmented them with citations to reports, statements and data series. When sources disagreed, we tried to present both the disagreement and the reasons behind it.

## From research to policy cards

The research was organised into policy groups (for example, energy, housing or welfare). That structure helps readers find related work and see connections across departments.

For each policy or incident, we produced a short entry (a “card”) with a consistent set of fields: a plain‑language intent, the main mechanisms, key claims and evidence, costs and funding, distributional effects, risks and constraints, a brief timeline, and an outcome score. Wherever we make a claim, we include an inline citation like `[^id]` and list the references in the citations file so they can also be exported inline.

## Scoring process

Every card has an outcome score on a simple five‑point scale from −2 to +2. The score is an informed judgement about the balance of likely benefits and harms at this point in time. It is not a prediction and it can change as delivery progresses.

- <span class="score-help score-2">`+2` means strong positive outcomes are visible or very likely if maintained.</span>
- <span class="score-help score-1">`+1` means modest positive outcomes or a sensible direction with caveats.</span>
- <span class="score-help score-0">`0` means unclear or balanced impacts; it is too early to tell.</span>
- <span class="score-help score--1">`−1` means modest negative outcomes or notable risks outweighing benefits.</span>
- <span class="score-help score--2">`−2` means serious negative outcomes or high‑risk choices with little offsetting value.</span>

Initial scores were generated automatically based on tagged evidence. Every score was then reviewed by a human editor for context and fairness. All scores remain open to revision as new evidence arrives or different perspectives are offered. Contributions and reasoned challenges are welcome via GitHub.

## Scoring consistency

A broader range of scores was considered, and would give more opportunity for nuance in complex issues, but assigning a subjective score to every policy means we have to make decisions about which influences and outcomes are more important than others (does the financial cost of national energy independence outweigh the national security offered by relying less on overseas gas?), so a narrow range keeps the system simple and clear.

A weighted approach was also considered (is nuclear energy capacity more or less important than consistent building regulations around single-sex spaces?), but that very quickly shows the limitations, and enhances the bias, rather than reduces it. We're not trying to make any one perspective more important than any other, but to consider the overall effects of decisions made and the plans laid out.

As our scoring is intended to be seen as comparable to better or worse governance systems, reducing the overall potential variance lets us define how broad this scale _could_ be, so we can focus on how and where it is relatively good (or not). Practically, our scale runs from "the worst any government could be expected to do" to "the best any government could be expected to do", given the range of issues tested, making it less a morality or political test, and more a test/scale of competence and effectiveness.

## Taxonomy and tags

To make the content filterable and to keep the write‑ups short, we use a small set of tags. These appear inline in square brackets.

Impact quality describes how confident we are in a claim:

- `[impact-proven]` is supported by direct evidence, delivered outputs or robust evaluation.
- `[impact-likely]` has strong rationale and early indicators but needs time to confirm.
- `[impact-hypothetical]` is a plausible mechanism with little or no current evidence.
- `[unknown]` flags a material uncertainty or missing data.
- `[opinion]` marks a clearly interpretive statement.

Areas of effect help group by domain. Common examples include: `[area: energy]`, `[area: water]`, `[area: housing]`, `[area: planning]`, `[area: nhs]`, `[area: social-care]`, `[area: welfare]`, `[area: education]`, `[area: skills]`, `[area: migration]`, `[area: justice]`, `[area: economy]`, `[area: fiscal]`, `[area: devolution]`, `[area: digital]`, `[area: ai]`, `[area: environment]`, `[area: business]`, `[area: labour-market]`, `[area: transport]`, `[area: culture]`, `[area: foreign]`, `[area: defence]`.

Time horizons describe when effects should become visible: `[horizon: short(<1y)]`, `[horizon: medium(1–3y)]`, and `[horizon: long(>3y)]`.

Distributional tags highlight who is most affected. For example: `[dist: low-income]`, `[dist: pensioners]`, `[dist: renters]`, `[dist: homeowners]`, `[dist: SMEs]`, `[dist: large-firms]`, and regional tags like `[dist: regions:Greater Manchester]`.

Risk and uncertainty tags call out the main constraints. We use `[risk: legal]`, `[risk: delivery]`, `[risk: finance]`, `[risk: political]` and `[risk: data-gap]` to signpost what might go wrong or what we do not yet know.

## Citations and references

We use inline footnotes for every material claim: `[^id]`. Multiple references can be listed in one footnote, for example: `[^rshe-guidance-2025; ^ofcom-roadmap-2023]`. Source registries and provenance notes live in the references section of the project so you can trace what we relied on and why.

</div>

## Build and outputs

Cards are stored as markdown‑as‑data. A small build pipeline compiles them into the public report and the summary table. That table offers an overview, but is not strictly analysis; it is a quick map that points you to the underlying cards.

Because the content is structured, we can add interactive features later (for example, filters by area, horizon or risk). If readers think those would be helpful, we will prioritise them.

## Style and voice

We aim for clarity over jargon, short paragraphs over dense blocks, and concrete examples over abstract claims. We focus on people and outcomes, not on point‑scoring. See the [Writing Voice & Style guide](./style-guide.html) for more detail on tone and presentation.

## Roadmap and open questions

Two near‑term improvements are on the list. 
- We plan a panel control that can expand and collapse inline citations so footnotes are readable in place.
- Ratings will be adjusted over time, and while updates may be limited, this report can develop into an ongoing 'report card' for Labour's government over time.
